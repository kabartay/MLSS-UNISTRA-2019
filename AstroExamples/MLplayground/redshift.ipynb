{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "<font size=10>https://tinyurl.com/bg2mlBoF</font><br/>\n",
    "<font size=1>https://www.dropbox.com/sh/0bmxic46xg5k6my/AAAVdt_AvCJZPfzjb8jmNvxda?dl=0</font>\n",
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "-->\n",
    "\n",
    "![Beginners Guide to Machine Learning in Astronomy](Intro.png \"Beginners Guide to Machine Learning in Astronomy\")\n",
    "\n",
    "![https://creativecommons.org/licenses/by-nc-sa/4.0/](https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png \"https://creativecommons.org/licenses/by-nc-sa/4.0/\")\n",
    "<font size=1>\n",
    "presented by Kai Polsterer, HITS gGmbH at Ecole Doctorale 182 in Strassbourg, France. Not taking responsibility for anything including proper functionality and content of links.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">1. Measuring redshift based on photometric measurements</font>\n",
    "<br/>\n",
    "<font size=4>\n",
    "Before starting with the actual machine learning part, we need an example application.\n",
    "    \n",
    "</font>\n",
    "\n",
    "<center><h1>How about estimating the distance of quasars?</h1></center>\n",
    "\n",
    "<br/><br/>\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "    \n",
    "read more about quasars\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Quasar\n",
    "* https://www.britannica.com/science/quasar\n",
    "* http://adsabs.harvard.edu/abs/1993ARA%26A..31..473A\n",
    "\n",
    "read more about photometric redshifts\n",
    "    \n",
    "* https://arxiv.org/abs/1805.12574\n",
    "</div>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "![Photometric Redshifts](PhotoZ.png \"Photometric Redshifts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">2. Machine Learning or<br/><br/><center>\"the quest of finding a good predictive model\"</center></font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "given ...\n",
    "\n",
    "\n",
    "* the variety of input data\n",
    "* the options to transform or preprocess the data\n",
    "* assumptions on noise and measurement errors\n",
    "* available algorithms and their hyperparameters\n",
    "* different similarity measures\n",
    "* different scoring and evaluation methods\n",
    "\n",
    "the question is ...\n",
    "\n",
    "* what is a predictive model?\n",
    "* what is a good model?\n",
    "* is the model proper generalizing?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">3. Classification with explicit criterion</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "Before we can start estimating distances, we have to learn how to classify ...\n",
    "</font>\n",
    "\n",
    "<br/><br/>\n",
    "<center><h1>star vs. galaxy</h1></center><br/><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "Let's use data from Sloan Digital Sky Servey(SDSS) to do some experiments.<br/><br/>\n",
    "</font>\n",
    "\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "    \n",
    "read more about SDSS data source\n",
    "    \n",
    "* https://www.sdss.org/dr14/algorithms/classify/\n",
    "* https://skyserver.sdss.org/dr14/en/help/browser/browser.aspx#&&history=description+SpecPhotoAll+U\n",
    "    \n",
    "Acknowledgments\n",
    "\n",
    "<font size=1>\n",
    "Funding for the Sloan Digital Sky Survey IV has been provided by the Alfred P. Sloan Foundation, the U.S. Department of Energy Office of Science, and the Participating Institutions. SDSS acknowledges support and resources from the Center for High-Performance Computing at the University of Utah. The SDSS web site is https://www.sdss.org/.\n",
    "\n",
    "SDSS is managed by the Astrophysical Research Consortium for the Participating Institutions of the SDSS Collaboration including the Brazilian Participation Group, the Carnegie Institution for Science, Carnegie Mellon University, the Chilean Participation Group, the French Participation Group, Harvard-Smithsonian Center for Astrophysics, Instituto de Astrofísica de Canarias, The Johns Hopkins University, Kavli Institute for the Physics and Mathematics of the Universe (IPMU) / University of Tokyo, the Korean Participation Group, Lawrence Berkeley National Laboratory, Leibniz Institut für Astrophysik Potsdam (AIP), Max-Planck-Institut für Astronomie (MPIA Heidelberg), Max-Planck-Institut für Astrophysik (MPA Garching), Max-Planck-Institut für Extraterrestrische Physik (MPE), National Astronomical Observatories of China, New Mexico State University, New York University, University of Notre Dame, Observatório Nacional / MCTI, The Ohio State University, Pennsylvania State University, Shanghai Astronomical Observatory, United Kingdom Participation Group, Universidad Nacional Autónoma de México, University of Arizona, University of Colorado Boulder, University of Oxford, University of Portsmouth, University of Utah, University of Virginia, University of Washington, University of Wisconsin, Vanderbilt University, and Yale University.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<font size=4>\n",
    "SDSS criterion:\n",
    "\n",
    ">The photometric pipeline version used for DR2 and later data (5_4) classifies objects as extended (“galaxy”) or point-like (“star”) based on the difference between the cmodel and PSF magnitude. An object is classified as extended if$$psfMag – cmodelMag > 0.145$$\n",
    ">\n",
    ">If satisfied, type is set to GALAXY for that band; otherwise, type is set to STAR. The global type objc_type is set according to the same criterion, applied to the summed fluxes from all bands in which the object is detected.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muha/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/muha/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'astroquery'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-afed62f7b4fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mastropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascii\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mascii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mastroquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSDSS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'astroquery'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import astropy.io.ascii as ascii\n",
    "from astroquery.sdss import SDSS\n",
    "\n",
    "def downloadData(filename):\n",
    "    object_query = \"select top 10000 specObjID, psfMag_g, psfMag_r, psfMag_i, cModelMag_g, cModelMag_r, cModelMag_i,  class from SpecPhotoAll where psfMag_g > 10 order by newid()\"\n",
    "    result = SDSS.query_sql(object_query, cache=False)\n",
    "\n",
    "    galaxyIndex = numpy.where(numpy.array(result['class'], dtype=str) == \"GALAXY\")[0]\n",
    "    starIndex = numpy.where(numpy.array(result['class'], dtype=str) == \"STAR\")[0]\n",
    "    qsoIndex = numpy.where(numpy.array(result['class'], dtype=str) == \"QSO\")[0]\n",
    "\n",
    "    result['class'][galaxyIndex] = 1\n",
    "    result['class'][starIndex] = -1\n",
    "    result['class'][qsoIndex] = -1\n",
    "    result = result.group_by('class')\n",
    "    \n",
    "    ascii.write(result, filename, format='csv')  \n",
    "\n",
    "if False: downloadData(\"sdss_point_vs_extended.csv\") #download data\n",
    "\n",
    "result = ascii.read('sdss_point_vs_extended.csv', format='csv')\n",
    "\n",
    "extended = numpy.where(numpy.array(result['class'].astype(int)) == 1)[0]\n",
    "point = numpy.where(numpy.array(result['class'].astype(int)) == -1)[0]\n",
    "\n",
    "colors = result['psfMag_r'] - result['cModelMag_r']\n",
    "\n",
    "pyplot.figure(figsize=(16,5))\n",
    "pyplot.hist(colors[extended], bins = 50, facecolor=\"r\", alpha=0.5, range=(-0.5,4))\n",
    "pyplot.hist(colors[point], bins = 50, facecolor=\"b\", alpha=0.5, range=(-0.5,4))\n",
    "pyplot.axvline(x=0.145, color=\"k\")\n",
    "pyplot.xlim(-0.5,4)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateError(threshold, colors, classes):\n",
    "    return (numpy.where((colors > threshold) & (classes == 1))[0].shape[0] +\\\n",
    "           numpy.where((colors <= threshold) & (classes == -1))[0].shape[0]) / colors.shape[0]\n",
    "    \n",
    "print (\"%2.2f%% correctly classified\" % (calculateError(0.145, colors, numpy.array(result['class'].astype(int)))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimalThreshold(threshold):\n",
    "    return 1-calculateError(threshold, colors, numpy.array(result['class'].astype(int)))\n",
    "\n",
    "bestThreshold = minimize(optimalThreshold, 0, method='nelder-mead').x\n",
    "\n",
    "print (\"%2.2f%% correctly classified with a threshold value of %f\" % ((calculateError(bestThreshold, colors, numpy.array(result['class'].astype(int)))*100) , bestThreshold))\n",
    "\n",
    "pyplot.figure(figsize=(16,5))\n",
    "pyplot.hist(colors[extended], bins = 50, facecolor=\"r\", alpha=0.5, range=(-0.5,4))\n",
    "pyplot.hist(colors[point], bins = 50, facecolor=\"b\", alpha=0.5, range=(-0.5,4))\n",
    "pyplot.axvline(x=0.145, color=\"k\")\n",
    "pyplot.axvline(x=bestThreshold, color=\"g\")\n",
    "pyplot.xlim(-0.5,4)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">4. Classification with reference data</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "We want to learn to classify by learning from data. Simple but powerful approach is to use a k-nearest-neighbor model.<br/>\n",
    "    \n",
    "$$\\hat Y(x) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i$$\n",
    "\n",
    "with $\\hat Y$ being the prediction for an input $x$, calculated as the dominant class in the class values $y_i$ of the $k$ most similar objects retrieved by the neighborhood function $N_k(x)$.\n",
    "</font>\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about k-nearewst neighbors\n",
    "\n",
    "* https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "* https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "* https://www.youtube.com/watch?v=UqYde-LULfs\n",
    "* https://www.coursera.org/lecture/big-data-machine-learning/k-nearest-neighbors-MamQ9\n",
    "\n",
    "read more about spatial neighborhood function implementation via kd-tree\n",
    "\n",
    "* https://en.wikipedia.org/wiki/K-d_tree\n",
    "* https://www.geeksforgeeks.org/k-dimensional-tree/\n",
    "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "\n",
    "numpy.random.seed(1234)\n",
    "\n",
    "n = 25\n",
    "marker = [\"x\",\"o\"]\n",
    "\n",
    "x1 = numpy.random.uniform(0,1,n)\n",
    "y1 = numpy.random.uniform(0,0.5,n)\n",
    "\n",
    "y2 = numpy.random.uniform(0.5,1,n)\n",
    "x2 = numpy.random.uniform(0,1,n)\n",
    "\n",
    "X = numpy.concatenate((x1,x2,y1,y2)).reshape(2,-1).T\n",
    "Y = numpy.concatenate((numpy.ones(n)*-1, numpy.ones(n)))\n",
    "\n",
    "pyplot.figure(figsize=(16,10))\n",
    "#pyplot.scatter(X[:,0],X[:,1],c=Y,s=100)\n",
    "pyplot.scatter(x1,y1,c=\"r\",marker=\"x\",s=100)\n",
    "pyplot.scatter(x2,y2,c=\"b\",marker=\"o\",s=100)\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "if False:\n",
    "    a = 0.7\n",
    "    b = 0.65\n",
    "    pyplot.scatter(a,b,c=\"g\",marker=\"*\",s=400)\n",
    "    \n",
    "    k = 5\n",
    "    myTree = KDTree(X, leafsize=3)\n",
    "    neighbors = myTree.query([a,b], k=k)\n",
    "    print (neighbors, Y[neighbors[1]])\n",
    "    #pyplot.scatter(X[:,0][neighbors[1]], X[:,1][neighbors[1]], marker=\"o\", s=400, alpha=0.5)\n",
    "    \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myTree = KDTree(X, leafsize=3)\n",
    "\n",
    "def getClass(x, k):\n",
    "    neighbors = myTree.query(x, k=k)\n",
    "    if numpy.mean(Y[neighbors[1]]) > 0.0:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "stepSize = 0.5\n",
    "coordinates = numpy.mgrid[0:1+stepSize:stepSize, 0:1+stepSize:stepSize].reshape(2,-1).T\n",
    "\n",
    "k = 5\n",
    "\n",
    "classes = [getClass(c, k) for c in coordinates]\n",
    "\n",
    "map = numpy.flipud(numpy.asarray(classes).reshape(int(1.0/stepSize) + 1,int(1.0/stepSize) + 1).T)\n",
    "\n",
    "pyplot.figure(figsize=(16,10))\n",
    "pyplot.scatter(x1,y1,c=\"r\",marker=\"x\",s=100)\n",
    "pyplot.scatter(x2,y2,c=\"b\",marker=\"o\",s=100)\n",
    "pyplot.imshow(map, extent=[0-stepSize/2.0,1+stepSize/2.0,0-stepSize/2.0,1+stepSize/2.0], interpolation=\"nearest\", alpha=0.2, aspect=\"auto\")\n",
    "\n",
    "pyplot.axhline(0.5,c=\"k\") # overfitting !!!!\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br/><br/>\n",
    "<font size=4>\n",
    "Let's go back to the real data example and use more dimensions than we could visualize.\n",
    "\n",
    "So many features to pick from:\n",
    "<br/><br/>\n",
    "    \n",
    "    \n",
    "* $mag_{psf}^g$, $mag_{psf}^r$, $mag_{psf}^i$\n",
    "* $mag_{model}^g$, $mag_{model}^r$, $mag_{model}^i$\n",
    "* $mag_{psf}^g - mag_{psf}^r$, $mag_{psf}^r - mag_{psf}^i$\n",
    "* $mag_{model}^g - mag_{model}^r$, $mag_{model}^r - mag_{model}^i$\n",
    "* $mag_{psf}^g - mag_{model}^g$, $mag_{psf}^r - mag_{model}^r$, $mag_{psf}^i - mag_{model}^i$\n",
    "</font>\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ascii.read('sdss_point_vs_extended.csv', format='csv')\n",
    "\n",
    "X = numpy.concatenate((numpy.array(data['psfMag_g'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['psfMag_r'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['psfMag_i'] - data['cModelMag_i']))).reshape(3,-1).T\n",
    "Y = numpy.array(data['class'])\n",
    "\n",
    "myTree = KDTree(X, leafsize=10)\n",
    "k = 1\n",
    "\n",
    "predictedClasses = numpy.array([getClass(x, k) for x in X])\n",
    "\n",
    "print ((1.0 - numpy.sum(numpy.abs(predictedClasses - Y)/2)/Y.shape[0]) * 100, \"% correctly classified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">5. Evaluation</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "We need a better way to evaluate the performance of our algorithm. We want to compare predicted with the real classes.</font><br/>\n",
    "\n",
    "<table><tr><td>\n",
    "    <font size=7>True Positive</font>\n",
    "    <font size=4><br/>objects from the +1 class correctly classified</font></td><td>\n",
    "    <font size=7>False Positive</font>\n",
    "    <font size=4><br/>objects from the +1 class NOT correctly classified</font></td></tr>\n",
    "  <tr><td>\n",
    "    <font size=7>False Negative</font>\n",
    "    <font size=4><br/>objects from the -1 class NOT correctly classified</font></td><td>\n",
    "    <font size=7>True Negative</font>\n",
    "    <font size=4><br/>objects from the -1 class correctly classified</font></td></tr></table>\n",
    "</font>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about simple class evaluation\n",
    "\n",
    "* https://en.wikipedia.org/wiki/False_positives_and_false_negatives\n",
    "* https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative\n",
    "* https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateResults(predictions, correctValues):\n",
    "    truePositive = numpy.where(predictions[numpy.where(correctValues == 1)[0]]==1)[0].shape\n",
    "    falsePositive = numpy.where(predictions[numpy.where(correctValues == 1)[0]]==-1)[0].shape\n",
    "    trueNegative = numpy.where(predictions[numpy.where(correctValues == -1)[0]]==-1)[0].shape\n",
    "    falseNegative = numpy.where(predictions[numpy.where(correctValues == -1)[0]]==1)[0].shape\n",
    "    return truePositive, falsePositive, falseNegative, trueNegative\n",
    "\n",
    "confusionMatrix = numpy.array(evaluateResults(predictedClasses, Y))\n",
    "print ((confusionMatrix/Y.shape[0]*100.0).reshape(2,2))\n",
    "#optimize k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><font size=4>\n",
    "Now let's simulate we observe data the next night and see how well we perform!</font><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False: downloadData(\"sdss_point_vs_extended_2.csv\") #download data\n",
    "    \n",
    "data = ascii.read('sdss_point_vs_extended_2.csv', format='csv')\n",
    "\n",
    "Xnew = numpy.concatenate((numpy.array(data['psfMag_g'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['psfMag_r'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['psfMag_i'] - data['cModelMag_i']))).reshape(3,-1).T\n",
    "Ynew = numpy.array(data['class'])\n",
    "\n",
    "predictedClasses = numpy.array([getClass(x, k) for x in Xnew])\n",
    "confusionMatrix = numpy.array(evaluateResults(predictedClasses, Ynew))\n",
    "\n",
    "print ((1.0 - numpy.sum(numpy.abs(predictedClasses - Ynew)/2)/Ynew.shape[0]) * 100, \"% correctly classified\")\n",
    "print ((confusionMatrix/Ynew.shape[0]*100.0).reshape(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">6. Validation</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "What went wrong? We used the same data for training and for evaluation.\n",
    "\n",
    "What is a good model? How can we properly validate, whether the model generalized correctly or not.\n",
    "\n",
    "Do exactly what we did in the experiment:\n",
    "\n",
    "* select data for training\n",
    "* select data for evaluating\n",
    "* make sure that those sets are independent ... think of observing the next night\n",
    "\n",
    "</font>\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about validation\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets\n",
    "* https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "* https://www.geeksforgeeks.org/cross-validation-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "</div>\n",
    "\n",
    "<font size=4>\n",
    "Let's go back to synthetic data!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(1234)\n",
    "\n",
    "n = 8\n",
    "\n",
    "x1 = numpy.concatenate((numpy.random.uniform(0,0.6,n//2),numpy.random.uniform(0.6,1,n//2)))\n",
    "y1 = numpy.concatenate((numpy.random.uniform(0,0.5,n//2),numpy.random.uniform(0.0,1,n//2))) \n",
    "y2 = numpy.random.uniform(0.5,1,n)\n",
    "x2 = numpy.random.uniform(0,0.6,n)\n",
    "\n",
    "x3 = numpy.concatenate((numpy.random.uniform(0,0.6,n//2),numpy.random.uniform(0.6,1,n//2)))\n",
    "y3 = numpy.concatenate((numpy.random.uniform(0,0.5,n//2),numpy.random.uniform(0.0,1,n//2))) \n",
    "y4 = numpy.random.uniform(0.5,1,n)\n",
    "x4 = numpy.random.uniform(0,0.6,n)\n",
    "\n",
    "Xtrain = numpy.concatenate((x1,x2,y1,y2)).reshape(2,-1).T\n",
    "Ytrain = numpy.concatenate((numpy.ones(n)*-1, numpy.ones(n)))\n",
    "\n",
    "Xvalidation = numpy.concatenate((x3,x4,y3,y4)).reshape(2,-1).T\n",
    "Yvalidation = numpy.concatenate((numpy.ones(n)*-1, numpy.ones(n)))\n",
    "\n",
    "k = 5\n",
    "myTree = KDTree(Xtrain, leafsize=3)\n",
    "\n",
    "def getClass(x, k):\n",
    "    neighbors = myTree.query(x, k=k)\n",
    "    if numpy.mean(Ytrain[neighbors[1]]) > 0.0:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "stepSize = 0.02\n",
    "coordinates = numpy.mgrid[0:1+stepSize:stepSize, 0:1+stepSize:stepSize].reshape(2,-1).T\n",
    "\n",
    "classes = [getClass(c, k) for c in coordinates]\n",
    "map = numpy.flipud(numpy.asarray(classes).reshape(int(1.0/stepSize) + 1,int(1.0/stepSize) + 1).T)\n",
    "\n",
    "predictedClasses = numpy.array([getClass(x, k) for x in Xvalidation])\n",
    "confusionMatrix = numpy.array(evaluateResults(predictedClasses, Yvalidation))\n",
    "\n",
    "pyplot.figure(figsize=(16,10))\n",
    "pyplot.imshow(map, extent=[0-stepSize/2.0,1+stepSize/2.0,0-stepSize/2.0,1+stepSize/2.0], interpolation=\"nearest\", alpha=0.2, aspect=\"auto\")\n",
    "pyplot.scatter(x1,y1,c=\"r\",marker=\"x\",s=100)\n",
    "pyplot.scatter(x2,y2,c=\"b\",marker=\"o\",s=100)\n",
    "pyplot.scatter(x3,y3,c=\"r\",marker=\"+\",s=100, alpha=0.5)\n",
    "pyplot.scatter(x4,y4,c=\"b\",marker=\"p\",s=100, alpha=0.5)\n",
    "#pyplot.scatter(Xvalidation[:,0],Xvalidation[:,1],c=predictedClasses, marker=\"o\", s=400, alpha=0.25)\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "print ((1.0 - numpy.sum(numpy.abs(predictedClasses - Yvalidation)/2)/Yvalidation.shape[0]) * 100, \"% correctly classified\")\n",
    "print ((confusionMatrix/Yvalidation.shape[0]*100.0).reshape(2,2))\n",
    "#optimize k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x5 = numpy.concatenate((numpy.random.uniform(0,0.6,n//2),numpy.random.uniform(0.6,1,n//2)))\n",
    "y5 = numpy.concatenate((numpy.random.uniform(0,0.5,n//2),numpy.random.uniform(0.0,1,n//2))) \n",
    "y6 = numpy.random.uniform(0.5,1,n)\n",
    "x6 = numpy.random.uniform(0,0.6,n)\n",
    "\n",
    "Xtest = numpy.concatenate((x5,x6,y5,y6)).reshape(2,-1).T\n",
    "Ytest = numpy.concatenate((numpy.ones(n)*-1, numpy.ones(n)))\n",
    "\n",
    "predictedClasses = numpy.array([getClass(x, k) for x in Xtest])\n",
    "confusionMatrix = numpy.array(evaluateResults(predictedClasses, Ytest))\n",
    "\n",
    "print ((1.0 - numpy.sum(numpy.abs(predictedClasses - Ytest)/2)/Ytest.shape[0]) * 100, \"% correctly classified\")\n",
    "print ((confusionMatrix/Ytest.shape[0]*100.0).reshape(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Let's apply it to the real data example and check the results!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ascii.read('sdss_point_vs_extended.csv', format='csv')\n",
    "\n",
    "Xnew = numpy.concatenate((numpy.array(data['psfMag_g'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['psfMag_r'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['psfMag_i'] - data['cModelMag_i']))).reshape(3,-1).T\n",
    "Ynew = numpy.array(data['class'])\n",
    "\n",
    "n = Ynew.shape[0]\n",
    "Xtrain = Xnew[:n//3]\n",
    "Ytrain = Ynew[:n//3]\n",
    "Xvalidate = Xnew[n//3:n//3*2]\n",
    "Yvalidate = Ynew[n//3:n//3*2]\n",
    "Xtest = Xnew[-n//3:]\n",
    "Ytest = Ynew[-n//3:]\n",
    "\n",
    "k = 5\n",
    "myTree = KDTree(Xtrain, leafsize=5)\n",
    "\n",
    "def getClass(x, k):\n",
    "    neighbors = myTree.query(x, k=k)\n",
    "    if numpy.mean(Ytrain[neighbors[1]]) > 0.0:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "predictedClasses = numpy.array([getClass(x, k) for x in Xvalidate])\n",
    "confusionMatrix = numpy.array(evaluateResults(predictedClasses, Yvalidate))\n",
    "print (\"validation:\")\n",
    "print ((1.0 - numpy.sum(numpy.abs(predictedClasses - Yvalidate)/2)/Yvalidate.shape[0]) * 100, \"% correctly classified\")\n",
    "print ((confusionMatrix/Yvalidate.shape[0]*100.0).reshape(2,2))\n",
    "\n",
    "predictedClasses = numpy.array([getClass(x, k) for x in Xtest])\n",
    "confusionMatrix = numpy.array(evaluateResults(predictedClasses, Ytest))\n",
    "print (\"testing:\")\n",
    "print ((1.0 - numpy.sum(numpy.abs(predictedClasses - Ytest)/2)/Ytest.shape[0]) * 100, \"% correctly classified\")\n",
    "print ((confusionMatrix/Ytest.shape[0]*100.0).reshape(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "What went wrong? We should use a better validation technique to figure out what happened. There are different techniques available. One of the most common ones in machine learning is:\n",
    "</font>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<center><h1>k-fold cross validation</h1></center><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "5-fold cross validation as an example:<br/>\n",
    "    \n",
    "<table style=\"font-size:20px;\">\n",
    "    <tr><td>run #1</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#fcc\">validate on this</td></tr>\n",
    "    <tr><td>run #2</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#fcc\">validate on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td></tr>\n",
    "    <tr><td>run #3</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#fcc\">validate on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td></tr>\n",
    "    <tr><td>run #4</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#fcc\">validate on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td></tr>\n",
    "    <tr><td>run #5</td>\n",
    "        <td bgcolor=\"#fcc\">validate on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td>\n",
    "        <td bgcolor=\"#ccf\">train on this</td></tr>\n",
    "</table>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = ascii.read('sdss_point_vs_extended.csv', format='csv')\n",
    "\n",
    "X = numpy.concatenate((numpy.array(data['psfMag_g'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['psfMag_r'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['psfMag_i'] - data['cModelMag_i']))).reshape(3,-1).T\n",
    "Y = numpy.array(data['class'])\n",
    "\n",
    "def getClass(tree, labels, x, k):\n",
    "    neighbors = tree.query(x, k=k)\n",
    "    if numpy.mean(labels[neighbors[1]]) > 0.0:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "kFold = KFold(n_splits=5, shuffle=False)\n",
    "k = 5\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "   \n",
    "    myTree = KDTree(Xtrain, leafsize=5)\n",
    "    predictedClasses = numpy.array([getClass(myTree, Ytrain, x, k) for x in Xvalidate])\n",
    "    confusionMatrix = numpy.array(evaluateResults(predictedClasses, Yvalidate))\n",
    "    results.append((1.0 - numpy.sum(numpy.abs(predictedClasses - Yvalidate)/2)/Yvalidate.shape[0]))\n",
    "    print (\"validation run %2d: %2.2f%% correctly classified\" %(i, results[-1] * 100))\n",
    "    print ((confusionMatrix/Yvalidate.shape[0]*100.0).reshape(2,2))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean performance: %2.2f%% standard deviation: %2.2f%%\" % (numpy.mean(results)*100.0, numpy.std(results)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<font size=4>\n",
    "We used ordered data with the point sources first. Therefore we did not have representative training data and we just evaluated the performance on extended sources. By enabling the shuffling, we make sure that the individual folds are randomly populated!\n",
    "    \n",
    "Next, let's use more features!\n",
    "</font>\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data = ascii.read('sdss_point_vs_extended.csv', format='csv')\n",
    "\n",
    "X = numpy.concatenate((numpy.array(data['psfMag_g'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['psfMag_r'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['psfMag_i'] - data['cModelMag_i']),\n",
    "                       numpy.array(data['psfMag_g'] - data['psfMag_r']),\n",
    "                       numpy.array(data['psfMag_r'] - data['psfMag_i']),\n",
    "                       numpy.array(data['cModelMag_g'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['cModelMag_r'] - data['cModelMag_i']),\n",
    "                       numpy.array(data['psfMag_g']),\n",
    "                       numpy.array(data['psfMag_r']),\n",
    "                       numpy.array(data['psfMag_i']),\n",
    "                       numpy.array(data['cModelMag_g']),\n",
    "                       numpy.array(data['cModelMag_r']),\n",
    "                       numpy.array(data['cModelMag_i']))).reshape(13,-1).T\n",
    "Y = numpy.array(data['class'])\n",
    "\n",
    "def getClass(tree, labels, x, k):\n",
    "    neighbors = tree.query(x, k=k)\n",
    "    if numpy.mean(labels[neighbors[1]]) > 0.0:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "k = 5\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "   \n",
    "    myTree = KDTree(Xtrain, leafsize=5)\n",
    "    predictedClasses = numpy.array([getClass(myTree, Ytrain, x, k) for x in Xvalidate])\n",
    "    confusionMatrix = numpy.array(evaluateResults(predictedClasses, Yvalidate))\n",
    "    results.append((1.0 - numpy.sum(numpy.abs(predictedClasses - Yvalidate)/2)/Yvalidate.shape[0]))\n",
    "    print (\"validation run %2d: %2.2f%% correctly classified\" %(i, results[-1] * 100))\n",
    "    print ((confusionMatrix/Yvalidate.shape[0]*100.0).reshape(2,2))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean performance: %2.2f%% standard deviation: %2.2f%%\" % (numpy.mean(results)*100.0, numpy.std(results)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True: #normalize the data\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:,i] = (X[:,i] - numpy.min(X[:,i])) / (numpy.max(X[:,i]) - numpy.min(X[:,i]))\n",
    "    \n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "k = 5\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "   \n",
    "    myTree = KDTree(Xtrain, leafsize=5)\n",
    "    predictedClasses = numpy.array([getClass(myTree, Ytrain, x, k) for x in Xvalidate])\n",
    "    confusionMatrix = numpy.array(evaluateResults(predictedClasses, Yvalidate))\n",
    "    results.append((1.0 - numpy.sum(numpy.abs(predictedClasses - Yvalidate)/2)/Yvalidate.shape[0]))\n",
    "    print (\"validation run %2d: %2.2f%% correctly classified\" %(i, results[-1] * 100))\n",
    "    print ((confusionMatrix/Yvalidate.shape[0]*100.0).reshape(2,2))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean performance: %2.2f%% standard deviation: %2.2f%%\" % (numpy.mean(results)*100.0, numpy.std(results)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">7. Unbalanced training data</font><br/><br/>\n",
    "\n",
    "\n",
    "<font size=4>\n",
    "There are to many extended source in comparison to compact sources in the sample. Think of 99% of class A and 1% of class B. Always predicting A should give 99% correct classifications, even though the model did not generalize.\n",
    "    \n",
    "Let's use a balanced data set and calculate some correlation coefficients!\n",
    "</font>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about correlation coefficients and unbalanced training sets\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\n",
    "* https://lettier.github.io/posts/2016-08-05-matthews-correlation-coefficient.html\n",
    "* https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "* https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6\n",
    "* https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculateMCC(confusionMatrix):\n",
    "    truePositive = confusionMatrix[0]\n",
    "    falsePositive = confusionMatrix[1]\n",
    "    falseNegative = confusionMatrix[2]\n",
    "    trueNegative = confusionMatrix[3]\n",
    "    return (truePositive*trueNegative - falsePositive*falseNegative) / \\\n",
    "           math.sqrt((truePositive+falsePositive)*(truePositive+falseNegative)*(trueNegative+falsePositive)*(trueNegative+falseNegative))\n",
    "\n",
    "print (calculateMCC(confusionMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from astropy.table import vstack\n",
    "\n",
    "def downloadBalancedData(filename):\n",
    "    galaxy_query = \"select top 5000 specObjID, psfMag_g, psfMag_r, psfMag_i, cModelMag_g, cModelMag_r, cModelMag_i,  class from SpecPhotoAll where psfMag_g > 10 and class='GALAXY' order by newid()\"\n",
    "    resultG = SDSS.query_sql(galaxy_query, cache=False)\n",
    "    quasar_query = \"select top 5000 specObjID, psfMag_g, psfMag_r, psfMag_i, cModelMag_g, cModelMag_r, cModelMag_i,  class from SpecPhotoAll where psfMag_g > 10 and (class='STAR' or class='QSO') order by newid()\"\n",
    "    resultQ = SDSS.query_sql(quasar_query, cache=False)\n",
    "    \n",
    "    result = vstack((resultG,resultQ))\n",
    "\n",
    "    galaxyIndex = numpy.where(numpy.array(result['class'], dtype=str) == \"GALAXY\")[0]\n",
    "    starIndex = numpy.where(numpy.array(result['class'], dtype=str) == \"STAR\")[0]\n",
    "    qsoIndex = numpy.where(numpy.array(result['class'], dtype=str) == \"QSO\")[0]\n",
    "\n",
    "    result['class'][galaxyIndex] = 1\n",
    "    result['class'][starIndex] = -1\n",
    "    result['class'][qsoIndex] = -1\n",
    "    result = result.group_by('class')\n",
    "    \n",
    "    ascii.write(result, filename, format='csv')\n",
    "    \n",
    "if False: downloadBalancedData(\"sdss_point_vs_extended_balanced.csv\") #download data\n",
    "\n",
    "data = ascii.read('sdss_point_vs_extended_balanced.csv', format='csv')\n",
    "\n",
    "X = numpy.concatenate((numpy.array(data['psfMag_g'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['psfMag_r'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['psfMag_i'] - data['cModelMag_i']),\n",
    "                       numpy.array(data['psfMag_g'] - data['psfMag_r']),\n",
    "                       numpy.array(data['psfMag_r'] - data['psfMag_i']),\n",
    "                       numpy.array(data['cModelMag_g'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['cModelMag_r'] - data['cModelMag_i']),\n",
    "                       numpy.array(data['psfMag_g']),\n",
    "                       numpy.array(data['psfMag_r']),\n",
    "                       numpy.array(data['psfMag_i']),\n",
    "                       numpy.array(data['cModelMag_g']),\n",
    "                       numpy.array(data['cModelMag_r']),\n",
    "                       numpy.array(data['cModelMag_i']))).reshape(13,-1).T\n",
    "Y = numpy.array(data['class'])\n",
    "\n",
    "if True:\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:,i] = (X[:,i] - numpy.min(X[:,i])) / (numpy.max(X[:,i]) - numpy.min(X[:,i]))\n",
    "    \n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "k = 5\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "mccs = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "   \n",
    "    myTree = KDTree(Xtrain, leafsize=10)\n",
    "    predictedClasses = numpy.array([getClass(myTree, Ytrain, x, k) for x in Xvalidate])\n",
    "    confusionMatrix = numpy.array(evaluateResults(predictedClasses, Yvalidate))\n",
    "    results.append((1.0 - numpy.sum(numpy.abs(predictedClasses - Yvalidate)/2)/Yvalidate.shape[0]))\n",
    "    mccs.append(calculateMCC(confusionMatrix))\n",
    "    print (\"validation run %2d: %2.2f%% correctly classified, MCC = %2.2f\" %(i, results[-1] * 100,mccs[-1]))\n",
    "    print ((confusionMatrix/Yvalidate.shape[0]*100.0).reshape(2,2))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean performance: %2.2f%% standard deviation: %2.2f%%\" % (numpy.mean(results)*100.0, numpy.std(results)*100.0))\n",
    "print (\"mean MCC: %2.2f standard deviation: %2.2f\" % (numpy.mean(mccs), numpy.std(mccs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">8. Regression with reference data</font><br/><br/>\n",
    "\n",
    "\n",
    "<font size=4>\n",
    "After we learned how to classify objects it's time to start estimating their redshift. Use the same algorithm but don't use a majority vote.\n",
    "\n",
    "We want to learn to estimate by learning from data. Simple but powerful approach is to use a k-nearest-neighbor model.\n",
    "\n",
    "$$\\hat Y(x) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i$$\n",
    "\n",
    "with $\\hat Y$ being the prediction for an input $x$, calculated as the mean of the values $y_i$ of the $k$ most similar objects retrieved by the neighborhood function $N_k(x)$.\n",
    "\n",
    "</font>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about nearest neighbor regression\n",
    "\n",
    "* https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "* https://www.coursera.org/lecture/ml-regression/k-nearest-neighbors-regression-8pPF9\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadRedshiftData(filename):\n",
    "    object_query = \"select top 10000 specObjID, psfMag_u, psfMag_g, psfMag_r, psfMag_i, psfMag_z, cModelMag_u, cModelMag_g, cModelMag_r, cModelMag_i, cModelMag_z, z from SpecPhotoAll where psfMag_g > 10 and class='QSO' and z<5 order by newid()\"\n",
    "    result = SDSS.query_sql(object_query, cache=False)\n",
    "    ascii.write(result, filename, format='csv')\n",
    "    \n",
    "if False: downloadRedshiftData(\"sdss_redshift.csv\") #download data\n",
    "\n",
    "data = ascii.read('sdss_redshift.csv', format='csv')\n",
    "X = numpy.concatenate((numpy.array(data['psfMag_g'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['psfMag_r'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['psfMag_i'] - data['cModelMag_i']),\n",
    "                       numpy.array(data['psfMag_u'] - data['psfMag_g']),\n",
    "                       numpy.array(data['psfMag_g'] - data['psfMag_r']),\n",
    "                       numpy.array(data['psfMag_r'] - data['psfMag_i']),\n",
    "                       numpy.array(data['psfMag_i'] - data['psfMag_z']),\n",
    "                       numpy.array(data['cModelMag_u'] - data['cModelMag_g']),\n",
    "                       numpy.array(data['cModelMag_g'] - data['cModelMag_r']),\n",
    "                       numpy.array(data['cModelMag_r'] - data['cModelMag_i']),\n",
    "                       numpy.array(data['cModelMag_i'] - data['cModelMag_z']),\n",
    "                       numpy.array(data['psfMag_r']),\n",
    "                       numpy.array(data['cModelMag_r']))).reshape(13,-1).T\n",
    "Y = numpy.array(data['z'])\n",
    "names = [\"psf_g-model_g\",\"psf_r-model_r\",\"psf_i-model_i\",\"psf_u-psf_g\",\"psf_g-psf_r\",\"psf_r-psf_i\",\"psf_i-psf_z\",\"model_u-model_g\",\"model_g-model_r\",\"model_r-model_i\",\"model_i-model_z\",\"psf_r\",\"model_r\"]\n",
    "\n",
    "if True:\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:,i] = (X[:,i] - numpy.min(X[:,i])) / (numpy.max(X[:,i]) - numpy.min(X[:,i]))\n",
    "        \n",
    "def getPrediction(tree, labels, x, k):\n",
    "    neighbors = tree.query(x, k=k)\n",
    "    return numpy.mean(labels[neighbors[1]])\n",
    "    \n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "k = 5\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "specZ = []\n",
    "photoZ = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "   \n",
    "    myTree = KDTree(Xtrain, leafsize=5)\n",
    "    predictedRedshifts = numpy.array([getPrediction(myTree, Ytrain, x, k) for x in Xvalidate])\n",
    "    results.append(numpy.sqrt(numpy.mean(numpy.square(predictedRedshifts - Yvalidate))))\n",
    "    specZ.append(Yvalidate)\n",
    "    photoZ.append(predictedRedshifts)\n",
    "    print (\"validation run %2d: RMSE: %2.2f\" %(i, results[-1]))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean RMSE: %2.2f standard deviation: %2.2f\" % (numpy.mean(results), numpy.std(results)))\n",
    "\n",
    "specZ = numpy.array(specZ).flatten()\n",
    "photoZ = numpy.array(photoZ).flatten()\n",
    "pyplot.figure(figsize=(10,10))\n",
    "pyplot.scatter(specZ, photoZ, alpha=0.1)\n",
    "pyplot.xlim(0,numpy.max(specZ))\n",
    "pyplot.ylim(0,numpy.max(specZ))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">9. How to further optimize - other algorithms / decision trees</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "After we learned about validation and scoring, there are many thinks to optimize:<br/>\n",
    "\n",
    "* features\n",
    "* distance function\n",
    "* weights\n",
    "* preprocessing\n",
    "* hyperparameters\n",
    "\n",
    "How about a different algorithm: Let's use decision trees.\n",
    "</font>\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about decision trees\n",
    "\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "* https://scikit-learn.org/stable/modules/tree.html\n",
    "* https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1\n",
    "* https://web.stanford.edu/~hastie/Papers/ESLII.pdf page 305[324]\n",
    "\n",
    "</div>\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "specZ = []\n",
    "photoZ = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "\n",
    "    myTree = DecisionTreeRegressor(max_depth=20)\n",
    "    myTree.fit(Xtrain, Ytrain)\n",
    "    predictedRedshifts = myTree.predict(Xvalidate)\n",
    "    results.append(numpy.sqrt(numpy.mean(numpy.square(predictedRedshifts - Yvalidate))))\n",
    "    specZ.append(Yvalidate)\n",
    "    photoZ.append(predictedRedshifts)\n",
    "    print (\"validation run %2d: RMSE: %2.2f\" %(i, results[-1]))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean RMSE: %2.2f standard deviation: %2.2f\" % (numpy.mean(results), numpy.std(results)))\n",
    "\n",
    "specZ = numpy.array(specZ).flatten()\n",
    "photoZ = numpy.array(photoZ).flatten()\n",
    "\n",
    "pyplot.figure(figsize=(10,10))\n",
    "pyplot.scatter(specZ, photoZ, alpha=0.1)\n",
    "pyplot.xlim(0,numpy.max(specZ))\n",
    "pyplot.ylim(0,numpy.max(specZ))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (numpy.sort(myTree.feature_importances_))\n",
    "print (numpy.array(names)[numpy.argsort(myTree.feature_importances_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">10. Ensemble learning - random forest</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "Let's use the results of multiple experts to reduce variance, by combine predictors with uncorrelated errors. In this case we will use an ensemble of decision trees also known as random forest.\n",
    "</font>\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "read more about ensemble<br/>\n",
    "\n",
    "* https://de.wikipedia.org/wiki/Ensemble_learning\n",
    "* https://en.wikipedia.org/wiki/Random_forest\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "* https://web.stanford.edu/~hastie/Papers/ESLII.pdf page 587[606], 605[624]\n",
    "* http://www.jmlr.org/papers/volume5/chawla04a/chawla04a.pdf\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "specZ = []\n",
    "photoZ = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "\n",
    "    myForest = RandomForestRegressor(n_estimators=164)\n",
    "    myForest.fit(Xtrain, Ytrain)\n",
    "\n",
    "    predictedRedshifts = myForest.predict(Xvalidate)\n",
    "    results.append(numpy.sqrt(numpy.mean(numpy.square(predictedRedshifts - Yvalidate))))\n",
    "    specZ.append(Yvalidate)\n",
    "    photoZ.append(predictedRedshifts)\n",
    "    print (\"validation run %2d: RMSE: %2.2f\" %(i, results[-1]))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean RMSE: %2.2f standard deviation: %2.2f\" % (numpy.mean(results), numpy.std(results)))\n",
    "\n",
    "specZ = numpy.array(specZ).flatten()\n",
    "photoZ = numpy.array(photoZ).flatten()\n",
    "pyplot.figure(figsize=(10,10))\n",
    "pyplot.scatter(specZ, photoZ, alpha=0.1)\n",
    "pyplot.xlim(0,numpy.max(specZ))\n",
    "pyplot.ylim(0,numpy.max(specZ))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">11. Doing science with all the things presented above</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "Using a GPU-based, kNN ensemble, multi-massive feature selection approach we found the following features to be very powerful. Just use the best 7 out of 10 for the next experiment: \n",
    "\n",
    "$$mag_{petro}^i/mag_{psf}^i$$\n",
    "$$\\hat {mag}_{psf}^g - \\hat {mag}_{model}^u$$\n",
    "$$\\hat {mag}_{exp}^i / \\hat {mag}_{psf}^r$$\n",
    "$$\\sqrt { {\\sigma_{model}^r}^2 + {\\sigma_{dev}^r}^2}$$\n",
    "$$\\hat {mag}_{psf}^r / \\hat {mag}_{exp}^g$$\n",
    "$$\\hat {mag}_{psf}^i / \\hat {mag}_{model}^z$$\n",
    "$${mag}_{psf}^i - {mag}_{dev}^i$$\n",
    "</font>\n",
    "\n",
    "<br/><br/>\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "    \n",
    "read more about feature selection\n",
    "\n",
    "* https://arxiv.org/abs/1803.10032\n",
    "* https://arxiv.org/abs/1310.1976\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "![Feature Selection](FeatureSelection.png \"Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadRedshiftDataFeatures(filename):\n",
    "    object_query = \"select top 10000 specObjID, (petroMag_i/psfMag_i) as fa, ((psfMag_g-extinction_g)-(modelMag_u-extinction_u)) as fb, ((cModelMag_i-extinction_i)/(psfMag_r-extinction_r)) as fc, sqrt(square(modelMagErr_r) + square(cModelMagErr_r)) as fd, ((psfMag_r-extinction_r)/(cModelMag_g-extinction_g)) as fe, ((psfMag_i-extinction_i)/(modelMag_z-extinction_z)) as ff, (psfMag_i-cModelMag_i) as fg, z from SpecPhotoAll where psfMag_g > 10 and class='QSO' and z<5 order by newid()\"\n",
    "    result = SDSS.query_sql(object_query, cache=False)\n",
    "    ascii.write(result, filename, format='csv')\n",
    "    \n",
    "if False: downloadRedshiftDataFeatures(\"sdss_redshift_best_features.csv\") #download data\n",
    "\n",
    "data = ascii.read('sdss_redshift_best_features.csv', format='csv')\n",
    "X = numpy.concatenate((numpy.array(data['fa']),numpy.array(data['fb']),numpy.array(data['fc']),numpy.array(data['fd']),numpy.array(data['fe']),numpy.array(data['ff']),numpy.array(data['fg']))).reshape(7,-1).T\n",
    "Y = numpy.array(data['z'])\n",
    "\n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "specZ = []\n",
    "photoZ = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "\n",
    "    myForest = RandomForestRegressor(n_estimators=64)\n",
    "    myForest.fit(Xtrain, Ytrain)\n",
    "\n",
    "    predictedRedshifts = myForest.predict(Xvalidate)\n",
    "    results.append(numpy.sqrt(numpy.mean(numpy.square(predictedRedshifts - Yvalidate))))\n",
    "    specZ.append(Yvalidate)\n",
    "    photoZ.append(predictedRedshifts)\n",
    "    print (\"validation run %2d: RMSE: %2.2f\" %(i, results[-1]))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean RMSE: %2.2f standard deviation: %2.2f\" % (numpy.mean(results), numpy.std(results)))\n",
    "\n",
    "specZ = numpy.array(specZ).flatten()\n",
    "photoZ = numpy.array(photoZ).flatten()\n",
    "pyplot.figure(figsize=(10,10))\n",
    "pyplot.scatter(specZ, photoZ, alpha=0.1)\n",
    "pyplot.xlim(0,numpy.max(specZ))\n",
    "pyplot.ylim(0,numpy.max(specZ))\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">12. Artificial Neural Networks</font><br/><br/>\n",
    "\n",
    "<font size=4>\n",
    "How about a different algorithm: Let's use artificial neural networks. In this example Multi Layer Perceptron (MLP).<br/>\n",
    "    \n",
    "Completely different to the methods before:\n",
    "\n",
    "* generalizing\n",
    "* concept of training<br/>\n",
    "  by updating weights and biases of the network - backpropagation\n",
    "* concept of loss-function\n",
    "* very quick to calculate once trained - very time consuming training\n",
    "\n",
    "\n",
    "many more hyper-parameter / options\n",
    "\n",
    "* number of layers\n",
    "* size of layers\n",
    "* activation functions\n",
    "\n",
    "\n",
    "* different training strategies / regularization\n",
    "\n",
    "</font>\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about artificial neural networks\n",
    "\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "* https://www.youtube.com/watch?v=aircAruvnKk\n",
    "* https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "* https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "* https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html\n",
    "* https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "\n",
    "</div>\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + numpy.exp(-x))\n",
    "\n",
    "def sigmoidDiff(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "xplot = numpy.arange(-10,10,0.1)\n",
    "pyplot.figure(figsize=(16,5))\n",
    "ax1 = pyplot.gca()\n",
    "ax1.scatter(xplot, sigmoid(xplot), c=\"r\") # numpy.tanh\n",
    "ax2 = ax1.twinx()\n",
    "ax2.scatter(xplot, sigmoidDiff(sigmoid(xplot)))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "myNet = MLPRegressor(hidden_layer_sizes = (15,7),\n",
    "                     activation='relu',#logistic, tanh, relu\n",
    "                     solver='sgd', #sgd, lbfgs\n",
    "                     learning_rate_init = 0.001,\n",
    "                     tol=1e-4,\n",
    "                     verbose=False)\n",
    "\n",
    "kFold = KFold(n_splits=5, shuffle=True)\n",
    "i = 1\n",
    "\n",
    "results = []\n",
    "specZ = []\n",
    "photoZ = []\n",
    "\n",
    "for trainIndex, validateIndex in kFold.split(X):\n",
    "    Xtrain, Xvalidate = X[trainIndex], X[validateIndex]\n",
    "    Ytrain, Yvalidate = Y[trainIndex], Y[validateIndex]\n",
    "\n",
    "    myNet.fit(Xtrain, Ytrain)\n",
    "\n",
    "    predictedRedshifts = myNet.predict(Xvalidate)\n",
    "    results.append(numpy.sqrt(numpy.mean(numpy.square(predictedRedshifts - Yvalidate))))\n",
    "    specZ.append(Yvalidate)\n",
    "    photoZ.append(predictedRedshifts)\n",
    "    print (\"validation run %2d: RMSE: %2.2f\" %(i, results[-1]))\n",
    "    i = i + 1\n",
    "print (\"####################################\")\n",
    "print (\"mean RMSE: %2.2f standard deviation: %2.2f\" % (numpy.mean(results), numpy.std(results)))\n",
    "\n",
    "specZ = numpy.array(specZ).flatten()\n",
    "photoZ = numpy.array(photoZ).flatten()\n",
    "pyplot.figure(figsize=(10,10))\n",
    "pyplot.scatter(specZ, photoZ, alpha=0.1)\n",
    "pyplot.xlim(0,numpy.max(specZ))\n",
    "pyplot.ylim(0,numpy.max(specZ))\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><font color=\"113388\" size=\"6\">13. What's next</font><br/><br/>\n",
    "\n",
    "<font size=4>you learned about:</font>\n",
    "    \n",
    "<center><h1>optimization is machine learning</h1></center>\n",
    "<center><h1>classification with explicit criterion</h1></center>\n",
    "<center><h1>classification with reference data</h1></center>\n",
    "<center><h1>regression with reference data</h1></center>\n",
    "<center><h1>evaluation</h1></center>\n",
    "<center><h1>validation</h1></center>\n",
    "<center><h1>unbalanced data</h1></center>\n",
    "<center><h1>ensemble strategies</h1></center>\n",
    "<center><h1>kNN, decision tree, random forest, artificial neural networks</h1></center><br/><br/>\n",
    "\n",
    "<font size=4>you can learn more about:</font>\n",
    "\n",
    "<center><h1>deep learning, recurrent neural networks, autoencoder</h1></center>\n",
    "<center><h1>unsupervised methods</h1><br/>\n",
    "clustering, dimensionality reduction, outlier detection, kernel methods</center>\n",
    "<center><h1>Bayesian methods / inference</h1></center>\n",
    "<center><h1>information theory</h1></center>\n",
    "<center><h1>scoring functions</h1></center>\n",
    "<center><h1>knowledge transfer / domain shift / active learning<br/>\n",
    "transfer learning / reinforcement learning<br/>\n",
    "    online learning / learning ... </h1></center>\n",
    "<br/><br/>\n",
    "\n",
    "<div style=\"background-color:#f0f0ff;padding: 10px;outline-color: #0000ff;outline-width: 1px;outline-style: solid;width:40%;position:relative;left:60%;\">\n",
    "\n",
    "read more about machine learning\n",
    "\n",
    "* https://web.stanford.edu/~hastie/Papers/ESLII.pdf\n",
    "* https://see.stanford.edu/course/cs229\n",
    "* http://lmgtfy.com/?q=machine+learning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = numpy.array([\"attention\", \"sleeping\", \"your\", \"not\", \"thanks\", \"boring\", \"is\", \"for\"])\n",
    "index = numpy.array([4,7,2,0])\n",
    "print (words[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
